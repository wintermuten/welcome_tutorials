{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial\n",
    "MILA, November 2017\n",
    "\n",
    "By Sandeep Subramanian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation (Seq2Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training, validation & test data\n",
    "\n",
    "Training data was obtained from http://www.manythings.org/anki/ and partitioned randomly into train, dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [line.strip().split('\\t') for line in codecs.open('data/jpn-train.txt', 'r', encoding='utf-8')]\n",
    "dev_lines = [line.strip().split('\\t') for line in codecs.open('data/jpn-dev.txt', 'r', encoding='utf-8')]\n",
    "test_lines = [line.strip().split('\\t') for line in codecs.open('data/jpn-test.txt', 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute source and target vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Japanese words : 2367 \n",
      "Number of unique English words : 16065 \n"
     ]
    }
   ],
   "source": [
    "src_vocab = set()\n",
    "trg_vocab = set()\n",
    "for line in train_lines:\n",
    "    for word in line[1]:\n",
    "        if word not in src_vocab:\n",
    "            src_vocab.add(word)\n",
    "    for word in line[0].split():\n",
    "        if word not in trg_vocab:\n",
    "            trg_vocab.add(word)\n",
    "\n",
    "# Add special tokens to the source and target vocabularies\n",
    "src_vocab.add('<s>')\n",
    "src_vocab.add('</s>')\n",
    "src_vocab.add('<unk>')\n",
    "src_vocab.add('<pad>')\n",
    "\n",
    "trg_vocab.add('<s>')\n",
    "trg_vocab.add('</s>')\n",
    "trg_vocab.add('<unk>')\n",
    "trg_vocab.add('<pad>')\n",
    "\n",
    "src_word2id = {word: idx for idx, word in enumerate(src_vocab)}\n",
    "src_id2word = {idx: word for idx, word in enumerate(src_vocab)}\n",
    "\n",
    "trg_word2id = {word: idx for idx, word in enumerate(trg_vocab)}\n",
    "trg_id2word = {idx: word for idx, word in enumerate(trg_vocab)}\n",
    "\n",
    "print('Number of unique Japanese words : %d ' % (len(src_vocab)))\n",
    "print('Number of unique English words : %d ' % (len(trg_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Seq2Seq model with GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"A Vanilla Sequence to Sequence (Seq2Seq) model with LSTMs.\n",
    "    Ref: Sequence to Sequence Learning with Neural Nets\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, src_emb_dim, trg_emb_dim, src_vocab_size,\n",
    "        trg_vocab_size, src_hidden_dim, trg_hidden_dim,\n",
    "        pad_token_src, pad_token_trg, bidirectional=False,\n",
    "        nlayers_src=1, nlayers_trg=1\n",
    "    ):\n",
    "        \"\"\"Initialize Seq2Seq Model.\"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.src_emb_dim = src_emb_dim\n",
    "        self.trg_emb_dim = trg_emb_dim\n",
    "        self.src_hidden_dim = src_hidden_dim\n",
    "        self.trg_hidden_dim = trg_hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.nlayers_src = nlayers_src\n",
    "        self.nlayers_trg = nlayers_trg\n",
    "        self.pad_token_src = pad_token_src\n",
    "        self.pad_token_trg = pad_token_trg\n",
    "        \n",
    "        # Word Embedding look-up table for the soruce language\n",
    "        self.src_embedding = nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            self.pad_token_src,\n",
    "        )\n",
    "\n",
    "        # Word Embedding look-up table for the target language\n",
    "        self.trg_embedding = nn.Embedding(\n",
    "            self.trg_vocab_size,\n",
    "            self.trg_emb_dim,\n",
    "            self.pad_token_trg,\n",
    "        )\n",
    "\n",
    "        # Encoder GRU\n",
    "        self.encoder = nn.GRU(\n",
    "            self.src_emb_dim // 2 if self.bidirectional else self.src_emb_dim,\n",
    "            self.src_hidden_dim,\n",
    "            self.nlayers_src,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Decoder GRU\n",
    "        self.decoder = nn.GRU(\n",
    "            self.trg_emb_dim,\n",
    "            self.trg_hidden_dim,\n",
    "            self.nlayers_trg,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layer from decoder hidden states to target language vocabulary\n",
    "        self.decoder2vocab = nn.Linear(trg_hidden_dim, trg_vocab_size)\n",
    "\n",
    "    def forward(self, input_src, input_trg, src_lengths):\n",
    "        # Lookup word embeddings in source and target minibatch\n",
    "        src_emb = self.src_embedding(input_src)\n",
    "        trg_emb = self.trg_embedding(input_trg)\n",
    "        \n",
    "        # Pack padded sequence for length masking in encoder RNN (This requires sorting input sequence by length)\n",
    "        src_emb = pack_padded_sequence(src_emb, src_lengths, batch_first=True)\n",
    "        \n",
    "        # Run sequence of embeddings through the encoder GRU\n",
    "        _, src_h_t = self.encoder(src_emb)\n",
    "        \n",
    "        # Extract the last hidden state of the GRU\n",
    "        h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1) if self.bidirectional else src_h_t[-1]\n",
    "\n",
    "        # Initialize the decoder GRU with the last hidden state of the encoder and \n",
    "        # run target inputs through the decoder.\n",
    "        trg_h, _ = self.decoder(trg_emb, h_t.unsqueeze(0).expand(self.nlayers_trg, h_t.size(0), h_t.size(1)))\n",
    "        \n",
    "        # Merge batch and time dimensions to pass to a linear layer\n",
    "        trg_h_reshape = trg_h.contiguous().view(\n",
    "            trg_h.size(0) * trg_h.size(1), trg_h.size(2)\n",
    "        )\n",
    "        \n",
    "        # Affine transformation of all decoder hidden states\n",
    "        decoder2vocab = self.decoder2vocab(trg_h_reshape)\n",
    "        \n",
    "        # Reshape\n",
    "        decoder2vocab = decoder2vocab.view(\n",
    "            trg_h.size(0), trg_h.size(1), decoder2vocab.size(1)\n",
    "        )\n",
    "\n",
    "        return decoder2vocab\n",
    "    \n",
    "    def decode(self, decoder2vocab):\n",
    "        # Turn decoder output into a probabiltiy distribution over vocabulary\n",
    "        decoder2vocab_reshape = decoder2vocab.view(-1, decoder2vocab.size(2))\n",
    "        word_probs = F.softmax(decoder2vocab_reshape)\n",
    "        word_probs = word_probs.view(\n",
    "            decoder2vocab.size(0), decoder2vocab.size(1), decoder2vocab.size(2)\n",
    "        )\n",
    "\n",
    "        return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parallel_minibatch(lines, src_word2id, trg_word2id, index, batch_size, volatile=False):\n",
    "        \n",
    "        # Get source sentences for this minibatch\n",
    "        src_lines = [\n",
    "            ['<s>'] + list(line[1]) + ['</s>']\n",
    "            for line in lines[index: index + batch_size]\n",
    "        ]\n",
    "\n",
    "        # Get target sentences for this minibatch\n",
    "        trg_lines = [\n",
    "            ['<s>'] + line[0].split() + ['</s>']\n",
    "            for line in lines[index: index + batch_size]\n",
    "        ]\n",
    "        \n",
    "        # Sort source sentences by length for length masking in RNNs\n",
    "        src_lens = [len(line) for line in src_lines]\n",
    "        sorted_indices = np.argsort(src_lens)[::-1]\n",
    "        \n",
    "        # Reorder sentences based on source lengths\n",
    "        sorted_src_lines = [src_lines[idx] for idx in sorted_indices]\n",
    "        sorted_trg_lines = [trg_lines[idx] for idx in sorted_indices]\n",
    "        \n",
    "        # Compute new sentence lengths\n",
    "        sorted_src_lens = [len(line) for line in sorted_src_lines]\n",
    "        sorted_trg_lens = [len(line) for line in sorted_trg_lines]\n",
    "        \n",
    "        # Get max source and target lengths to pad input and output sequences\n",
    "        max_src_len = max(sorted_src_lens)\n",
    "        max_trg_len = max(sorted_trg_lens)\n",
    "        \n",
    "        # Construct padded source input sequence\n",
    "        input_lines_src = [\n",
    "            [src_word2id[w] if w in src_word2id else src_word2id['<unk>'] for w in line] +\n",
    "            [src_word2id['<pad>']] * (max_src_len - len(line))\n",
    "            for line in sorted_src_lines\n",
    "        ]\n",
    "\n",
    "        # Construct padded target input sequence\n",
    "        input_lines_trg = [\n",
    "            [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[:-1]] +\n",
    "            [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "            for line in sorted_trg_lines\n",
    "        ]\n",
    "\n",
    "        # Construct padded target output sequence (Note: Output sequence is just the input shifted by 1 position)\n",
    "        # This is for teacher-forcing\n",
    "        output_lines_trg = [\n",
    "            [trg_word2id[w] if w in trg_word2id else trg_word2id['<unk>'] for w in line[1:]] +\n",
    "            [trg_word2id['<pad>']] * (max_trg_len - len(line))\n",
    "            for line in sorted_trg_lines\n",
    "        ]\n",
    "\n",
    "        input_lines_src = Variable(torch.LongTensor(input_lines_src), volatile=volatile)\n",
    "        input_lines_trg = Variable(torch.LongTensor(input_lines_trg), volatile=volatile)\n",
    "        output_lines_trg = Variable(torch.LongTensor(output_lines_trg), volatile=volatile)\n",
    "\n",
    "        return {\n",
    "            'input_src': input_lines_src,\n",
    "            'input_trg': input_lines_trg,\n",
    "            'output_trg': output_lines_trg,\n",
    "            'src_lens': sorted_src_lens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(\n",
    "    src_emb_dim=128, trg_emb_dim=128,\n",
    "    src_vocab_size=len(src_word2id), trg_vocab_size=len(trg_word2id),\n",
    "    src_hidden_dim=512, trg_hidden_dim=512,\n",
    "    pad_token_src=src_word2id['<pad>'],\n",
    "    pad_token_trg=trg_word2id['<pad>'],\n",
    ")\n",
    "\n",
    "if cuda_available:\n",
    "    seq2seq = seq2seq.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq.parameters(), lr=4e-4)\n",
    "weight_mask = torch.ones(len(trg_word2id))\n",
    "if cuda_available:\n",
    "    weight_mask = weight_mask.cuda()\n",
    "weight_mask[trg_word2id['<pad>']] = 0\n",
    "loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Training Loss : 5.496\n",
      "Epoch : 0 Dev Loss : 4.714\n",
      "Epoch : 0 Test Loss : 4.660\n",
      "-------------------------------------------------------------\n",
      "Epoch : 1 Training Loss : 4.218\n",
      "Epoch : 1 Dev Loss : 4.171\n",
      "Epoch : 1 Test Loss : 4.082\n",
      "-------------------------------------------------------------\n",
      "Epoch : 2 Training Loss : 3.594\n",
      "Epoch : 2 Dev Loss : 3.873\n",
      "Epoch : 2 Test Loss : 3.759\n",
      "-------------------------------------------------------------\n",
      "Epoch : 3 Training Loss : 3.122\n",
      "Epoch : 3 Dev Loss : 3.669\n",
      "Epoch : 3 Test Loss : 3.538\n",
      "-------------------------------------------------------------\n",
      "Epoch : 4 Training Loss : 2.731\n",
      "Epoch : 4 Dev Loss : 3.533\n",
      "Epoch : 4 Test Loss : 3.382\n",
      "-------------------------------------------------------------\n",
      "Epoch : 5 Training Loss : 2.397\n",
      "Epoch : 5 Dev Loss : 3.428\n",
      "Epoch : 5 Test Loss : 3.259\n",
      "-------------------------------------------------------------\n",
      "Epoch : 6 Training Loss : 2.110\n",
      "Epoch : 6 Dev Loss : 3.367\n",
      "Epoch : 6 Test Loss : 3.185\n",
      "-------------------------------------------------------------\n",
      "Epoch : 7 Training Loss : 1.866\n",
      "Epoch : 7 Dev Loss : 3.318\n",
      "Epoch : 7 Test Loss : 3.111\n",
      "-------------------------------------------------------------\n",
      "Epoch : 8 Training Loss : 1.650\n",
      "Epoch : 8 Dev Loss : 3.283\n",
      "Epoch : 8 Test Loss : 3.063\n",
      "-------------------------------------------------------------\n",
      "Epoch : 9 Training Loss : 1.458\n",
      "Epoch : 9 Dev Loss : 3.257\n",
      "Epoch : 9 Test Loss : 3.029\n",
      "-------------------------------------------------------------\n",
      "Epoch : 10 Training Loss : 1.288\n",
      "Epoch : 10 Dev Loss : 3.267\n",
      "Epoch : 10 Test Loss : 3.025\n",
      "-------------------------------------------------------------\n",
      "Epoch : 11 Training Loss : 1.142\n",
      "Epoch : 11 Dev Loss : 3.287\n",
      "Epoch : 11 Test Loss : 3.042\n",
      "-------------------------------------------------------------\n",
      "Epoch : 12 Training Loss : 1.015\n",
      "Epoch : 12 Dev Loss : 3.290\n",
      "Epoch : 12 Test Loss : 3.052\n",
      "-------------------------------------------------------------\n",
      "Epoch : 13 Training Loss : 0.897\n",
      "Epoch : 13 Dev Loss : 3.325\n",
      "Epoch : 13 Test Loss : 3.078\n",
      "-------------------------------------------------------------\n",
      "Epoch : 14 Training Loss : 0.784\n",
      "Epoch : 14 Dev Loss : 3.326\n",
      "Epoch : 14 Test Loss : 3.062\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    losses = []\n",
    "    for j in range(0, len(train_lines), batch_size):\n",
    "        # Get minibatch of examples\n",
    "        minibatch = get_parallel_minibatch(\n",
    "            lines=train_lines, src_word2id=src_word2id,\n",
    "            trg_word2id=trg_word2id, index=j, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        if cuda_available:\n",
    "            minibatch['input_src'] = minibatch['input_src'].cuda()\n",
    "            minibatch['input_trg'] = minibatch['input_trg'].cuda()\n",
    "            minibatch['output_trg'] = minibatch['output_trg'].cuda()\n",
    "        \n",
    "        decoder_out = seq2seq(\n",
    "            input_src=minibatch['input_src'], input_trg=minibatch['input_trg'], src_lengths=minibatch['src_lens']\n",
    "        )\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            minibatch['output_trg'].contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm(seq2seq.parameters(), 5.)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data[0])\n",
    "    \n",
    "    dev_nll = []\n",
    "    for j in range(0, len(dev_lines), batch_size):\n",
    "        # Get minibatch of examples\n",
    "        minibatch = get_parallel_minibatch(\n",
    "            lines=dev_lines, src_word2id=src_word2id,\n",
    "            trg_word2id=trg_word2id, index=j, batch_size=batch_size,\n",
    "            volatile=True\n",
    "        )\n",
    "        \n",
    "        if cuda_available:\n",
    "            minibatch['input_src'] = minibatch['input_src'].cuda()\n",
    "            minibatch['input_trg'] = minibatch['input_trg'].cuda()\n",
    "            minibatch['output_trg'] = minibatch['output_trg'].cuda()\n",
    "        \n",
    "        decoder_out = seq2seq(\n",
    "            input_src=minibatch['input_src'], input_trg=minibatch['input_trg'], src_lengths=minibatch['src_lens']\n",
    "        )\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            minibatch['output_trg'].contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        dev_nll.append(loss.data[0])\n",
    "    \n",
    "    test_nll = []\n",
    "    for j in range(0, len(test_lines), batch_size):\n",
    "        # Get minibatch of examples\n",
    "        minibatch = get_parallel_minibatch(\n",
    "            lines=test_lines, src_word2id=src_word2id,\n",
    "            trg_word2id=trg_word2id, index=j, batch_size=batch_size,\n",
    "            volatile=True\n",
    "        )\n",
    "        \n",
    "        if cuda_available:\n",
    "            minibatch['input_src'] = minibatch['input_src'].cuda()\n",
    "            minibatch['input_trg'] = minibatch['input_trg'].cuda()\n",
    "            minibatch['output_trg'] = minibatch['output_trg'].cuda()\n",
    "        \n",
    "        decoder_out = seq2seq(\n",
    "            input_src=minibatch['input_src'], input_trg=minibatch['input_trg'], src_lengths=minibatch['src_lens']\n",
    "        )\n",
    "        \n",
    "        loss = loss_criterion(\n",
    "            decoder_out.contiguous().view(-1, decoder_out.size(2)),\n",
    "            minibatch['output_trg'].contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        test_nll.append(loss.data[0])\n",
    "    \n",
    "    print('Epoch : %d Training Loss : %.3f' % (epoch, np.mean(losses)))\n",
    "    print('Epoch : %d Dev Loss : %.3f' % (epoch, np.mean(dev_nll)))\n",
    "    print('Epoch : %d Test Loss : %.3f' % (epoch, np.mean(test_nll)))\n",
    "    print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see what the model produces for a few sentences in our dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : Tom can't Tom unlikely unlikely that Tom would involved permitted of he would ever able to see the room. \n",
      "Gold : I think it's highly unlikely that Tom was not aware that he wouldn't be allowed to enter the museum without his parents. \n",
      "---------------\n",
      "Prediction : I desire I me that people for of his life \n",
      "Gold : The photo brought back many happy memories of my childhood. \n",
      "---------------\n",
      "Prediction : I thought a find a work of I was to the \n",
      "Gold : I expected to make new friends when I moved to Boston. \n",
      "---------------\n",
      "Prediction : He never heard him before. \n",
      "Gold : I've never heard him complaining about his meals. \n",
      "---------------\n",
      "Prediction : I thought you would be a to a to go out \n",
      "Gold : I thought it would be fun for us to go skiing together. \n",
      "---------------\n",
      "Prediction : The old was out of the way to the me to the station. \n",
      "Gold : The man went out of his way to take me to the station. \n",
      "---------------\n",
      "Prediction : Someone must have taken the umbrella in mistake. \n",
      "Gold : Someone must have taken my umbrella by mistake. \n",
      "---------------\n",
      "Prediction : It could surprised that she should such a thing. offer. \n",
      "Gold : I am surprised that she refused such a good offer. \n",
      "---------------\n",
      "Prediction : Tom has never been in to do a \n",
      "Gold : Tom has never been able to beat me. \n",
      "---------------\n",
      "Prediction : He and I money and was \n",
      "Gold : Everyone loved the <unk> I baked yesterday. \n",
      "---------------\n",
      "Prediction : I got a book for Tom. \n",
      "Gold : I've ordered a book from Amazon.com. \n",
      "---------------\n",
      "Prediction : The factory on the of the in day. \n",
      "Gold : The factory produces thousands of bottles every month. \n",
      "---------------\n",
      "Prediction : The are the earth's are is a \n",
      "Gold : Three-fourths of the earth's surface is covered with water. \n",
      "---------------\n",
      "Prediction : He a the boy but he father \n",
      "Gold : He's not a doctor, but a nurse. \n",
      "---------------\n",
      "Prediction : I'll be here late tomorrow this afternoon. \n",
      "Gold : She'll be up and around this afternoon. \n",
      "---------------\n",
      "Prediction : The sun was in warmer for the with. \n",
      "Gold : The situation was getting difficult to deal with. \n",
      "---------------\n",
      "Prediction : I told him to get for company. \n",
      "Gold : I told him to leave the room. \n",
      "---------------\n",
      "Prediction : She has a every her new in day. \n",
      "Gold : She spends time with her grandmother every Sunday. \n",
      "---------------\n",
      "Prediction : Tom has no no little with his \n",
      "Gold : Tom hasn't had a fight with anybody lately. \n",
      "---------------\n",
      "Prediction : He asked me to give the the salt. \n",
      "Gold : He asked me to pass him the salt. \n",
      "---------------\n",
      "Prediction : She is always a letter from \n",
      "Gold : She is writing a letter now. \n",
      "---------------\n",
      "Prediction : Take careful to to eat too much. \n",
      "Gold : Be sure not to eat too much. \n",
      "---------------\n",
      "Prediction : It was not a lot success. \n",
      "Gold : It was only a partial success. \n",
      "---------------\n",
      "Prediction : We will never fail \n",
      "Gold : We will never agree. \n",
      "---------------\n",
      "Prediction : She looked after her sight of her dog but \n",
      "Gold : She froze at the sight of the bear. \n",
      "---------------\n",
      "Prediction : My dog is \n",
      "Gold : Your reputation <unk> you. \n",
      "---------------\n",
      "Prediction : I need some favor I a \n",
      "Gold : I have a gift for you. \n",
      "---------------\n",
      "Prediction : The sudden of people became in front middle \n",
      "Gold : A bunch of people died in the explosion. \n",
      "---------------\n",
      "Prediction : We chartered a new dictionary. \n",
      "Gold : We bought a round table. \n",
      "---------------\n",
      "Prediction : Tom ate up after the late. \n",
      "Gold : Tom showed up 15 minutes late. \n",
      "---------------\n",
      "Prediction : There is a exceptions. \n",
      "Gold : Everything has its limit. \n",
      "---------------\n",
      "Prediction : Tom doesn't like chess. \n",
      "Gold : Tom doesn't like cheese. \n",
      "---------------\n",
      "Prediction : I go out again. \n",
      "Gold : Don't go there now. \n",
      "---------------\n",
      "Prediction : Will you leave a car? \n",
      "Gold : Can you ride a bicycle? \n",
      "---------------\n",
      "Prediction : She has a eye for beauty. beautiful. \n",
      "Gold : She has an eye for the beautiful. \n",
      "---------------\n",
      "Prediction : It was a dark cold day. \n",
      "Gold : It was a very beautiful flower. \n",
      "---------------\n",
      "Prediction : You him by \n",
      "Gold : Give him time. \n",
      "---------------\n",
      "Prediction : Could me the salt, please? you? \n",
      "Gold : Pass me the salt, will you? \n",
      "---------------\n",
      "Prediction : They have a bottles of wine. \n",
      "Gold : They drank two bottles of wine. \n",
      "---------------\n",
      "Prediction : Where about going to a moment. \n",
      "Gold : How about going for a swim? \n",
      "---------------\n",
      "Prediction : He made his drive for the left. \n",
      "Gold : He moved the desk to the right. \n",
      "---------------\n",
      "Prediction : Someone is ready. \n",
      "Gold : Dinner is ready. \n",
      "---------------\n",
      "Prediction : Do you know him? he is? \n",
      "Gold : Do you know who he is? \n",
      "---------------\n",
      "Prediction : I have up my diary every day. \n",
      "Gold : I write in my diary every day. \n",
      "---------------\n",
      "Prediction : People matter of people \n",
      "Gold : No <unk> allowed. \n",
      "---------------\n",
      "Prediction : He is afraid to the plan. \n",
      "Gold : He is familiar with the subject. \n",
      "---------------\n",
      "Prediction : You hope you could \n",
      "Gold : I wish you success. \n",
      "---------------\n",
      "Prediction : He changed a sleepy. \n",
      "Gold : He got very drunk. \n",
      "---------------\n",
      "Prediction : Tom's work is two of bed. \n",
      "Gold : Tom's clothes are out of fashion. \n",
      "---------------\n",
      "Prediction : I don't like there. trouble. it. place. of thing. \n",
      "Gold : I don't go in for that sort of thing. \n",
      "---------------\n",
      "Prediction : She is a selfish beauty. \n",
      "Gold : She is a real beauty. \n",
      "---------------\n",
      "Prediction : Speech is silver, silence for golden. \n",
      "Gold : Speech is silver, silence is gold. \n",
      "---------------\n",
      "Prediction : Tom is been high right \n",
      "Gold : Tom has a hangover. \n",
      "---------------\n",
      "Prediction : I am embarrassed. \n",
      "Gold : I was <unk> \n",
      "---------------\n",
      "Prediction : I'm a youngest in to the family. \n",
      "Gold : I'm the youngest child in the family. \n",
      "---------------\n",
      "Prediction : Don't it easy. \n",
      "Gold : Take it <unk> \n",
      "---------------\n",
      "Prediction : Don't touch mad. at me. \n",
      "Gold : Don't be mad at me. \n",
      "---------------\n",
      "Prediction : I have a fever. fever. \n",
      "Gold : I have a high temperature. \n",
      "---------------\n",
      "Prediction : He has a bad heart. \n",
      "Gold : He has a bad heart. \n",
      "---------------\n",
      "Prediction : What're are you doing? \n",
      "Gold : What are you doing? \n",
      "---------------\n",
      "Prediction : It's already 7 o'clock. \n",
      "Gold : It's already nine o'clock. \n",
      "---------------\n",
      "Prediction : Get out. \n",
      "Gold : Get out! \n",
      "---------------\n",
      "Prediction : Where is I? \n",
      "Gold : Where am I? \n",
      "---------------\n",
      "Prediction : Seriously? are you? \n",
      "Gold : How are you? \n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Get the first minibatch in the dev set.\n",
    "minibatch = get_parallel_minibatch(\n",
    "    lines=dev_lines, src_word2id=src_word2id,\n",
    "    trg_word2id=trg_word2id, index=0, batch_size=batch_size,\n",
    "    volatile=True\n",
    ")\n",
    "\n",
    "if cuda_available:\n",
    "    minibatch['input_src'] = minibatch['input_src'].cuda()\n",
    "    minibatch['input_trg'] = minibatch['input_trg'].cuda()\n",
    "    minibatch['output_trg'] = minibatch['output_trg'].cuda()\n",
    "\n",
    "# Run it through our model (in teacher forcing mode)\n",
    "res = seq2seq(\n",
    "    input_src=minibatch['input_src'], input_trg=minibatch['input_trg'], src_lengths=minibatch['src_lens']\n",
    ")\n",
    "\n",
    "# Pick the most likely word at each time step\n",
    "res = res.data.cpu().numpy().argmax(axis=-1)\n",
    "\n",
    "# Cast targets to numpy\n",
    "gold = minibatch['output_trg'].data.cpu().numpy()\n",
    "\n",
    "# Decode indices to words for predictions and gold\n",
    "res = [[trg_id2word[x] for x in line] for line in res]\n",
    "gold = [[trg_id2word[x] for x in line] for line in gold]\n",
    "\n",
    "for r, g in zip(res, gold):\n",
    "    if '</s>' in r:\n",
    "        index = r.index('</s>')\n",
    "    else:\n",
    "        index = len(r)\n",
    "    \n",
    "    print('Prediction : %s ' % (' '.join(r[:index])))\n",
    "\n",
    "    index = g.index('</s>')\n",
    "    print('Gold : %s ' % (' '.join(g[:index])))\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
